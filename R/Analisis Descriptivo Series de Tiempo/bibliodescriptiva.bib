@BOOK{Akaike1973,
	AUTHOR={Akaike, H.},
	TITLE={{Information theory and an extension of maximum likelihood principle.}},
    PUBLISHER={Springer},
    YEAR={1973},
    EDITION={First},
    PAGES={199-213}
}


@BOOK{BrocwellDavis2006,
	AUTHOR = {Brockwell, P.J. and Davis. R.A.},
	TITLE = {{Time Series: Theory and Methods}},
	PUBLISHER = {Springer},
	YEAR = {2006},
	edition = {Second},
}

@BOOK{BrocwellDavis2016,
	AUTHOR = {Brockwell, P.J. and Davis. R.A.},
	TITLE = {{Introduction to Time Series and Forecasting}},
	PUBLISHER = {Springer},
	YEAR = {2016},
	edition = {Third},
}

@ARTICLE{Brown1975,
	AUTHOR={Brown, R.L. Durbin, J. and Evans, J.M},
	TITLE={Techniques for Testing the Constancy of Regression Relationships over Time},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	year = {1975},
	volume={37},
	number={2},
	pages={149-192}    
}



@BOOK{DeGooijer2017,
	AUTHOR = {De Gooijer, J.G.},
	TITLE = {{Elements of Nonlinear Time Series Analysis
	and Forecasting}},
	PUBLISHER = {Springer},
	YEAR = {2017},
	edition = {First},
}


@BOOK{Dixon2020,
	AUTHOR = {Dixon, M.F. Halpering, I. and Bilokon, P.},
	TITLE = {{Machine Learning in Finance From Therory to Practice}},
	PUBLISHER = {Springer},
	YEAR = {2020},
	edition = {First},
}

@BOOK{DoucMouSto2014,
	AUTHOR = {Douc, R. Moulines, E. and Stoffer, D.S.},
	TITLE = {{Nonlinear Time Series: Theory, Methods, and Applications with R Examples.}},
	PUBLISHER = {CRC Press},
	YEAR = {2014},
	edition = {First},
}


@BOOK{Dong2018,
	AUTHOR = {Dong, G. and Liu, H.},
	TITLE = {{FEATURE ENGINEERING FOR MACHINE LEARNING AND DATA ANALYTICS.}},
	PUBLISHER = {CRC Press},
	YEAR = {2018},
	edition = {First},
}

@BOOK{Durbin2012,
	AUTHOR = {Durbin, J. and Koopman, D.S.},
	TITLE = {{Time Series Analysis by State Space Methods.}},
	PUBLISHER = {Oxford University Press},
	YEAR = {2012},
	edition = {Second},
}

@ARTICLE{Edgerton1994,
	AUTHOR={Edgerton, D. and Wells, C.},
	TITLE={Critical Values for the CUSUMSQ Statistic in
	Medium and Large Sized Samples},
	journal= {Oxford Bulletin of Economics and Statistics},
	year = {1994},
	volume={56},
	number={3},
	pages={355-365}    
}

@ARTICLE{Friedman2001,
	AUTHOR={Friedman, J.},
	TITLE={Greedy function approximation: A gradient boosting machine},
	journal= {Annals of Statistics},
	year = {2001},
	volume={29},
	number={},
	pages={1189-1232}    
}



@BOOK{Fuller1996,
	AUTHOR = {Fuller, W.A.},
	TITLE = {{Introduction to Statistical Time Series.}},
	PUBLISHER = {Wiley Series in Probability and Statistics},
	YEAR = {1996},
	edition = {Second},
}

@BOOK{GaussianHilbert1997,
	AUTHOR = {Janson, S. },
	TITLE = {{Gaussian Hilbert Spaces.}},
	PUBLISHER = {Cambridge University Press},
	YEAR = {1997},
	edition = {First},
}
@book{Goodfellow2016,
	title={Deep Learning},
	author={Goodfellow, I.  Bengio, Y. and Courville, A.},
	publisher={MIT Press, Cambridge, Massachusetts},
	year={2016}
}

@ARTICLE{Gneiting2011,
	AUTHOR={Gneiting, T.},
	TITLE={Making and Evaluating Point Forecasts},
	journal= {Journal of the American Statistical Association},
	year = {2011},
	volume={106},
	number={494},
	pages={746-762}    
}

@BOOK{Horn2013,
	AUTHOR = {Horn, R.A. and Jhonson, C.R.},
	TITLE = {{Matrix Analysis}},
	PUBLISHER = {Cambridge University Press},
	YEAR = {2013},
	edition = {Second},
}


@ARTICLE{Hornik1989,
	AUTHOR={Hornik, M. Stinchcombe, M. and White, H. },
	TITLE={Multilayer feedforward networks are
	universal approximators},
	journal= {Neural Networks},
	year = {1989},
	volume={2},
	number={5},
	pages={359-366}    
}

@article{Hochreiter1997,
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	title = "{Long Short-Term Memory}",
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	month = {11},
	abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@book{Hyndman2021,
	title     = "Forecasting: principles and practice",
	author    = "Hyndman, R.J. and Athanasopoulos, G.",
	year      = 2021,
	publisher = "OTexts: Melbourne, Australia",
	url       = "OTexts.com/fpp3"
}



@misc{kohler2020rate,
	title={On the rate of convergence of a deep recurrent neural network estimate in a regression problem with dependent data}, 
	author={Michael Kohler and Adam Krzyzak},
	year={2020},
	eprint={2011.00328},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@ARTICLE{Kratsios2021,
	AUTHOR={Kratsios, A. },
	TITLE={ The Universal Approximation Property.},
	journal= {Annals of Mathematics and Artificial Intelligence},
	year = {2021},
	volume={89},
	number={ },
	pages={435-469}    
}

@BOOK{Krispin2019,
	AUTHOR = {Krispin, Rami.},
	TITLE = {{Hands-On Time Series Analysis with R}},
	PUBLISHER = {Packt Publishing},
	YEAR = {2019},
	edition = {First},
}

@BOOK{Kuhn2020,
	AUTHOR = {Kuhn, M. and Johnson, K.},
	TITLE = {{Feature Engineering and Selection	A Practical Approach for Predictive Models}},
	PUBLISHER = {CRC Press},
	YEAR = {2020},
	edition = {First},
}

@BOOK{Huffaker2017,
	AUTHOR = {Huffaker, R.  Bitelli M. and Rosa, R.},
	TITLE = {{Nonlinear Time Series with R}},
	PUBLISHER = {Oxfor University Press},
	YEAR = {2017},
	edition = {First},
}

@BOOK{Lazzeri2020,
	AUTHOR = {Lazzeri, Francesca.},
	TITLE = {{Machine Learning for Time Series Forecasting with Python}},
	PUBLISHER = {John Wiley and Sons, Inc.},
	YEAR = {2020},
	edition = {First},
}

@BOOK{Lutkepohl2005,
	AUTHOR = {L{\"u}tkepohl, H.},
	TITLE = {{New Introduction to Multiple Time Series Analysis.}},
	PUBLISHER = {Springer},
	YEAR = {2005},
	edition = {First},
}


@BOOK{McKinney2018,
	AUTHOR = {McKinney, Wes.},
	TITLE = {{Python for Data Analysis}},
	PUBLISHER = {O’Reilly Media},
	YEAR = {2018},
	edition = {Second},
}

@book{Murphypml2022,
	author = "Kevin P. Murphy",
	title = "Probabilistic Machine Learning: An introduction",
	publisher = "MIT Press",
	year = 2022,
	url = "probml.ai"
}




@BOOK{Pena2010,
	AUTHOR = {Peña, D.},
	TITLE = {{Análisis de Series de Temporales.}},
	PUBLISHER = {Alianza Editorial},
	YEAR = {2010},
	edition = {Segunda},
}

@BOOK{Pena2021,
	AUTHOR = {Peña, D. and Tsay, R.},
	TITLE = {{Statistical Learning for Big Dependent Data.}},
	PUBLISHER = {Jhon Wiley and Sons, Inc.},
	YEAR = {2021},
	edition = {First},
}




@BOOK{Pollock1999,
	AUTHOR = {Pollock, D.S.G.},
	TITLE = {{A Handbook of Time-Series Analysis, Signal Processing and Dynamics.}},
	PUBLISHER = {Academic Press},
	YEAR = {1999},
	edition = {First},
}

@BOOK{Ritcher2021,
	AUTHOR = {Ritcher, S.},
	TITLE = {{Statistical Analysis of Machine Learning Algorithms}},
	PUBLISHER = {Universit{\"a}t Heidelberg},
	YEAR = {2021},
	EDITION={First}
}

@BOOK{Ritcher2019,
	AUTHOR = {Ritcher, S.},
	TITLE = {{Statistisches und maschinelles Lernen}},
	PUBLISHER = {Springer Spektrum},
	YEAR = {2019},
	EDITION={First}
}



@BOOK{Tib2013,
	AUTHOR = {James, G. Witten, D. and Hastie, T. and Tibshirani, R.},
	TITLE = {{An Introduction to Statistical Learning with Application in R.}},
	PUBLISHER = {Springer},
	YEAR = {2013},
	edition = {First},
}

@BOOK{Tib2009,
	AUTHOR = {Hastie, T. Tibshirani, R. and Friedman, J.},
	TITLE = {{The Elements of Statistical Learning Data Mining, Inference, and Prediction}},
	PUBLISHER = {Springer},
	YEAR = {2009},
	edition = {Second},
}

@BOOK{Tsay2010,
	AUTHOR = {Tsay, R.S.},
	TITLE = {{Analysis of Financial Time Series}},
	PUBLISHER = {Wiley Series},
	YEAR = {2010},
	edition = {Third},
}

@BOOK{Tsay2019,
	AUTHOR = {Tsay, R.S. and Chen, R.},
	TITLE = {{Nonlinear Time Series Analysis}},
	PUBLISHER = {Wiley Series},
	YEAR = {2019},
	edition = {First},
}

@BOOK{Tsay2014,
	AUTHOR = {Tsay, R.S.},
	TITLE = {{Multivariate Time Series Analysis with R and Financial Applications}},
	PUBLISHER = {Wiley Series},
	YEAR = {2014},
	edition = {First},
}


@ARTICLE{Schafer2006,
	AUTHOR={Sch\"{a}fer A.M., Zimmermann H.G.},
	TITLE={ Recurrent Neural Networks Are Universal Approximators},
	journal= {Artificial Neural Networks – ICANN 2006. ICANN 2006. Lecture Notes in Computer Science},
	year = {2006},
	volume={4131},
	number={},
	pages={632-640},
	doi={https://doi.org/10.1007/11840817_66}    
}


@BOOK{Shumway2017,
	AUTHOR = {Shumway, Robert H.  and Stoffer, David S. },
	TITLE = {{Time Series Analysis and Its Applications
	With R Examples.}},
	PUBLISHER = {Springer},
	YEAR = {2017},
	edition = {Fourth},
}

@BOOK{Vapnik1998,
	AUTHOR = {Vapnik, V. N. },
	TITLE = {{Statistical learning theory}},
	PUBLISHER = {Wiley-Interscience},
	YEAR = {1998},
	edition = {First},
}

@BOOK{Vapnik2000,
	AUTHOR = {Vapnik, V. N. },
	TITLE = {{The Nature of	Statistical Learning Theory}},
	PUBLISHER = {Springer},
	YEAR = {2000},
	edition = {Second},
}

@BOOK{Wei2019,
	AUTHOR = {Wei, W.S.},
	TITLE = {{Multivariate Time Series Analysis and Applications}},
	PUBLISHER = {Wiley},
	YEAR = {2019},
	edition = {First},
}

@ARTICLE{Yeo2000,
	AUTHOR={Yeo, I. and Jhonson R.A.},
	TITLE={A new family of power transformations to improve normality or symmetry.},
	journal= {Biometrika},
	year = {2000},
	volume={87},
	number={4},
	pages={954–959}
}

@ARTICLE{Hmamouche2017,
	AUTHOR={Hmamouche, Y. and  Przymus, P. and Casali, A. and Lakhal, L.},
	TITLE={GFSM: a Feature Selection Method for Improving Time Series Forecasting.},
	journal= {International Journal on Advances in Systems and Measurements},
	year = {2017},
	volume={10},
	number={3-4},
	pages={255-264}
}




